1. The result table is in output.tsv
2. In part 2, several variations are implemented, in which stemming didn't give a dramatic improvement on the accuracy. Removing stopword could slightly imporve the accuracy for "tank" and "plant" but did poorly on "pers/place" compared with the case without removing the stopword. Position weightings including the smooth exponential distance decay, stepped weighting and the customed weighting could slightly improve the accuracy. The bigram model applied the the adjacency of the target word decreased the accuracy.
For the customed position weighting, I gave 100 times more weight to immediate adjacent words, 90 times more weight to one-word-away words, 80 to two-word-away words ... 10 to nine-word-away words than others.
In the bayes model, preprocessings including stemming and removing stopword are applied. TF-IDF is still applied to acquire the word vectors. Smooth exponential distance decay and bag-of-words are adopted. The accuracy is comparable to the previous models.
The best model might be that without stemming and with stopword removed, using stepped weighting and bag-of-words. The bayes model performs equally well.
3. The vector models for targets tasks is different from that for non-targeted tasks in that it assumes the different weightings for words adjacent to the target word (ambiguous word) from others. Therefore, the weighting for the words in targets tasks is sensitive to the distance from the target word. The vector models for non-targeted tasks doesn't have this assumption and assumed uniform weighting.
4. In the treatment of smsspam, sent_imdb, sent_yelp, health, I assume no special position weighting but uniform weighting, no special local collocation modelling but bag-of-words. So you can think of the corresponding settings in each row with those datasets are invalid. I also added <s> and </s> to the stopwords. I use tf-idf as the default term weighting and cosine similarity as the default similarity because cosine similarity performs much better than other similarity measures.
5. The models didn't do well on the additional datasets. This is because the models can hardly decide the sentiment and authorship simply based on individual words. Instead, it should take the sentences or even paragraph as the basic unit to train the model and make prediction.   

