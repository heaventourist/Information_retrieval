From the result of the permutation experiment, we can conclude that tfidf weighting is the best for term weighting. The tf weighting and boolean weighting are almost comparable, indicating that the term frequency is not the dominant factor for acheiving a high precision. Stemming and stopwords exclusion didn't improve the precision a lot. Cosine is the best similarity measure. The different region weighting didn't make a lot different on the precision.
The result of the experiment proved that the document frequency is an important factor in deciding the term weights. Stopwords exclusion may not be appropriate in this experiment because it may reduce the precison of the query. Stemming is just a crude heuristic process that chops off the ends of words. Lemmatization might give a better performance in most cases. Most of the time we can not know the best similarity measure in advance, so it is necessary to try different measures before deciding which one to use. In most cases, region weighting should make a difference from unanimous weighting, but in this experiment, the terms in different regions did not bring a lot of differentiations. It's possible that the query might not use those hight weighting terms such that the weights of different regions is not in the full exploit.
To make a conclusion, parameter permutation may not always bring the desired effect. It is highly recommended to understand the characteristic of the documents before designing the model for information retrieval.

SVD extension:
The SVD processing is extremely slow, so I just gave the results when using tfidf to weight the terms and cosine to evaluate the similarity. The results is almost comparable to the original result.
